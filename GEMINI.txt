# api_client.py (Финальная версия с Session)

import requests
import time
import sys
from typing import List, Dict, Any

# Импортируем необходимые компоненты из rich
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn
from rich.panel import Panel
from rich.text import Text
from rich.theme import Theme
from rich.align import Align

# Максимально простая тема (без изменений)
custom_theme = Theme({
    "info": "dim",
    "success": "bold white",
    "danger": "bold red",
    "title": "bold white",
    "details": "dim",
    "highlight": "bold white",
    "url": "bold white underline"
})
console = Console(highlight=False, theme=custom_theme)

# <--- ИЗМЕНЕНИЕ: Функция теперь принимает объект session
def ping_server(session: requests.Session, api_url: str) -> bool:
    """Пингует сервер, чтобы убедиться, что он доступен, используя сессию."""
    with console.status(f"Pinging server at {api_url}...", spinner="dots"):
        try:
            # <--- ИЗМЕНЕНИЕ: Используем session.get
            response = session.get(f"{api_url}/ping", timeout=5)
            if response.status_code == 200 and response.json().get("message") == "pong":
                console.print("[success]OK[/success] Server is online.")
                return True
            else:
                console.print(f"[danger]FAIL[/danger] Server responded with status {response.status_code}.")
                return False
        except requests.exceptions.RequestException:
            console.print(f"[danger]FAIL[/danger] Server is offline or unreachable.")
            return False

# <--- ИЗМЕНЕНИЕ: Функция теперь принимает объект session
def show_available_loras(session: requests.Session, api_url: str):
    """Запрашивает и выводит список доступных LoRA, используя сессию."""
    console.rule("[title]Available LoRAs[/title]", style="dim")
    try:
        # <--- ИЗМЕНЕНИЕ: Используем session.get и добавляем таймаут
        response = session.get(f"{api_url}/loras", timeout=10)
        response.raise_for_status()
        loras = response.json()

        if not loras:
            console.print("[info]No LoRAs defined in the manifest on the server.[/info]")
            return

        table = Table(box=None, show_header=True, header_style="bold white", expand=True, padding=(0, 1))
        table.add_column("Name", style="white", no_wrap=True, ratio=30)
        table.add_column("Description", ratio=50)
        table.add_column("Triggers", style="dim", ratio=20)

        for i, lora in enumerate(loras):
            prefix = lora.get('prefix')
            suffix = lora.get('suffix')
            triggers = []
            if prefix: triggers.append(prefix)
            if suffix: triggers.append(f"..., {suffix}")
            trigger_str = "".join(triggers) if len(triggers) > 1 else (triggers[0] if triggers else "")
            
            end_section = True if i == len(loras) - 1 else False
            table.add_row(lora['name'], lora['description'], trigger_str, end_section=end_section)
        
        console.print(table)

    except requests.exceptions.RequestException as e:
        console.print(f"[danger]Could not fetch LoRA list: {e}[/danger]")

def print_task_details(title: str, payload: dict):
    """Форматирует и выводит детали задачи в виде центрированной панели."""
    # (Без изменений)
    params = payload.get("params", {})
    model = params.get("model", "N/A")
    steps = params.get("steps", "N/A")
    seed = params.get("seed", "N/A")
    prompt = params.get("prompt", "")
    
    info_text = Text()
    info_text.append("Model: ", style="details")
    info_text.append(f"{model}\n")
    info_text.append("Steps: ", style="details")
    info_text.append(f"{steps}\n")
    info_text.append("Seed:  ", style="details")
    info_text.append(f"{seed}\n")

    lora = params.get("lora")
    if lora and lora.lower() != "none":
        lora_strength = params.get("lora_strength", "N/A")
        info_text.append("LoRA:  ", style="details")
        info_text.append(f"{lora} (Strength: {lora_strength})\n")

    info_text.append("Prompt:", style="details")
    info_text.append(f' "{prompt}"')

    console.print(Align.center(
        Panel(info_text, title=f"[title]{title}[/title]", border_style="dim", width=100)
    ))

# <--- ИЗМЕНЕНИЕ: Функция теперь принимает объект session
def generate_and_wait(session: requests.Session, api_url: str, payload: dict, title: str = "Running Task"):
    """Отправляет задачу и отслеживает ее выполнение, используя сессию."""
    print_task_details(title, payload)
    
    total_start_time = time.time()
    try:
        console.print("[info]Submitting task to the server...[/info]")

        response = session.post(f"{api_url}/generate", json=payload, timeout=10)
        
        if not response.ok:
                    console.print(f"\n[danger]CRITICAL: Failed to submit task (Status: {response.status_code})[/danger]")
                    try:
                        error_details = response.json().get("detail", response.text)
                        console.print(f"[danger]Server message: {error_details}[/danger]")
                    except Exception:
                        console.print(f"[danger]Server response: {response.text}[/danger]")
                    return None

        task_id = response.json()['task_id']
        console.print(f"[info]Task submitted with ID:[/info] [highlight]{task_id}[/highlight]")

        final_result_data = None
        with Progress(
            TextColumn("[dim]{task.description}[/dim]"),
            BarColumn(bar_width=None, style="dim", complete_style="white"),
            TextColumn("[dim]{task.percentage:>3.0f}%[/dim]"),
            TextColumn("[dim]Elapsed:[/dim]"),
            TimeElapsedColumn(),
            console=console,
            transient=True
        ) as progress:
            
            task_progress = progress.add_task("Waiting...", total=100)

            while True:
                # <--- ИЗМЕНЕНИЕ: Используем session.get и добавляем таймаут
                status_response = session.get(f"{api_url}/tasks/{task_id}", timeout=5)
                status_response.raise_for_status()
                result_data = status_response.json()
                status = result_data.get("status")

                if status == "SUCCESS" or status == "FAILURE":
                    final_result_data = result_data
                    progress.update(task_progress, completed=100, description="[bold white]Finished[/bold white]")
                    break

                elif status == "PROGRESS":
                    progress_info = result_data.get("progress", {})
                    percent = progress_info.get("percent", 0)
                    progress.update(task_progress, completed=percent, description="Generating")

                elif status == "PENDING":
                    progress.update(task_progress, description="In queue")
                
                time.sleep(1)
        
        if final_result_data:
            total_elapsed_time = time.time() - total_start_time
            is_success = final_result_data.get("status") == "SUCCESS"
            status_tag = "[OK]" if is_success else "[FAIL]"
            console.print(f"{status_tag} Task finished in {total_elapsed_time:.2f} seconds.")
            return final_result_data
    
    except requests.exceptions.Timeout:
        console.print("\n[danger]Network or API error: The request timed out. The server might be busy or unresponsive.[/danger]")
    except requests.exceptions.RequestException as e:
        console.print(f"\n[danger]Network or API error: {e}[/danger]")
    except KeyboardInterrupt:
        console.print(f"\n[info]User interrupted the process. Exiting.[/info]")
    except Exception as e:
        console.print(f"\n[danger]An unexpected error occurred: {e}[/danger]")
    
    return None

if __name__ == "__main__":
    API_URL = "http://127.0.0.1:8000"

    # <--- КЛЮЧЕВОЕ ИЗМЕНЕНИЕ: Создаем объект Session ОДИН РАЗ ---
    with requests.Session() as session:
        # <--- ИЗМЕНЕНИЕ: Передаем session в функции
        if not ping_server(session, API_URL):
            sys.exit(1)

        show_available_loras(session, api_url=API_URL)
        
        console.print() 

        payload = {
            "workflow_id": "flux_default",
            "params": {
                "prompt": "sketch of a yellow hugging face emoji with big hands, minimalist, impressionism, negative space, flat beige background",
                "model": "flux1-dev-Q4_K_S.gguf",
                "steps": 20,
                "lora": "Flux-Ghibli-Art-LoRA.safetensors",
                "lora_strength": 0.9,
                "seed": "random"
            }
        }
        
        result = generate_and_wait(session, API_URL, payload, title="Generation Task")
        
        if result and result.get("status") == "SUCCESS":
            url = result['result']['download_url']
            console.print(f"   [info]Download URL:[/info] [url]{url}[/url]")
        elif result:
            console.print(f"   [danger]Details:[/danger] {result.get('result', 'No details')}")

    console.print("\n[title]Client finished.[/title]")


---


# client_minimal.py (Отказоустойчивая версия с Session)

import requests
import time
import sys

# <--- ИЗМЕНЕНИЕ: Функция теперь принимает объект session
def ping_server(session: requests.Session, api_url: str) -> bool:
    """Проверяет, доступен ли сервер, используя сессию."""
    print(f"Pinging {api_url}...")
    try:
        # <--- ИЗМЕНЕНИЕ: Используем session.get и добавляем таймаут
        response = session.get(f"{api_url}/ping", timeout=5)
        response.raise_for_status() # Проверка на ошибки 4xx/5xx
        print("Server is online.")
        return True
    except requests.exceptions.RequestException as e:
        print(f"Server is offline or unreachable: {e}")
        return False

# <--- ИЗМЕНЕНИЕ: Функция теперь принимает объект session
def generate_and_wait_minimal(session: requests.Session, api_url: str, payload: dict):
    """Отправляет задачу и отслеживает ее выполнение с простым прогресс-баром."""
    
    print("\n--- Task Details ---")
    params = payload.get("params", {})
    print(f"  Model: {params.get('model', 'N/A')}")
    print(f"  Steps: {params.get('steps', 'N/A')}")
    print(f'  Prompt: "{params.get("prompt", "")[:80]}..."')
    print("--------------------")

    total_start_time = time.time()
    try:
        print("Submitting task...")
        # <--- ИЗМЕНЕНИЕ: Используем session.post и добавляем таймаут
        response = session.post(f"{api_url}/generate", json=payload, timeout=10)
        if not response.ok: # .ok это True для статусов 2xx
            print(f"\nError: Failed to submit task (Status: {response.status_code})")
            try:
                # Пытаемся извлечь детальную ошибку из JSON-ответа сервера
                error_details = response.json().get("detail", response.text)
                print(f"Server message: {error_details}")
            except Exception:
                # Если ответ не JSON, выводим как текст
                print(f"Server response: {response.text}")
            return None # Завершаем функцию, так как задача не была создана
        ##response.raise_for_status()
        task_id = response.json()['task_id']
        print(f"Task submitted with ID: {task_id}")

        last_percent = -1
        while True:
            # <--- ИЗМЕНЕНИЕ: Используем session.get и добавляем таймаут
            status_response = session.get(f"{api_url}/tasks/{task_id}", timeout=5)
            status_response.raise_for_status()
            result_data = status_response.json()
            
            status = result_data.get("status")

            if status == "SUCCESS" or status == "FAILURE":
                total_elapsed_time = time.time() - total_start_time
                sys.stdout.write("\r" + " " * 80 + "\r")
                sys.stdout.flush()
                
                print(f"Task finished in {total_elapsed_time:.2f} seconds.")
                return result_data

            elif status == "PROGRESS":
                progress_info = result_data.get("progress", {})
                percent = progress_info.get("percent", 0)
                
                if int(percent) != int(last_percent):
                    bar_length = 40
                    filled_length = int(bar_length * percent / 100)
                    bar = '█' * filled_length + '-' * (bar_length - filled_length)
                    
                    sys.stdout.write(f"\rProgress: [{bar}] {percent:.0f}%")
                    sys.stdout.flush()
                    last_percent = percent

            elif status == "PENDING":
                sys.stdout.write("\rTask is waiting in queue...")
                sys.stdout.flush()
            
            time.sleep(1)

    except requests.exceptions.Timeout:
        print("\nError: The request timed out. The server might be busy or unresponsive.")
    except requests.exceptions.RequestException as e:
        print(f"\nError: Network or API issue - {e}")
    except KeyboardInterrupt:
        print("\nProcess interrupted by user.")
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")
    
    return None


if __name__ == "__main__":
    API_URL = "http://127.0.0.1:8000"

    # <--- КЛЮЧЕВОЕ ИЗМЕНЕНИЕ: Создаем объект Session ОДИН РАЗ ---
    with requests.Session() as session:
        # <--- ИЗМЕНЕНИЕ: Передаем session в функцию
        if not ping_server(session, API_URL):
            sys.exit(1)

        # Оригинальная полезная нагрузка из файла client_minimal.py
        payload = {
            "workflow_id": "flux_default",
            "params": {
                "prompt": "A beautiful landscape painting, epic sky, masterpiece",
                "model": "flux1-schnell-Q4_K_S.gguf",
                "steps": 4,
                "seed": "random"
            }
        }
        
        # <--- ИЗМЕНЕНИЕ: Передаем session в функцию
        result = generate_and_wait_minimal(session, API_URL, payload)
        
        if result and result.get("status") == "SUCCESS":
            print(f"Download URL: {result['result']['download_url']}")
        elif result:
            print(f"Failure Details: {result.get('result', 'No details')}")
        else:
            print("Task execution failed or was interrupted.")

    print("\nClient finished.")


---

# .env - Configuration for local development and testing.


# --- FastAPI & Network Settings ---
# The API server will only be accessible from your local machine.
UVICORN_HOST="0.0.0.0"
UVICORN_PORT="8000"

# This IP is used to generate download URLs. For local testing, it must be localhost.
PUBLIC_IP="127.0.0.1"

# --- Celery & Redis Settings ---
# All services will connect to Redis on localhost.
REDIS_HOST="localhost"
REDIS_PORT="6379"
REDIS_PASSWORD="redis"
REDIS_DB="0"

# Get your Hugging Face token here: https://huggingface.co/settings/tokens
# This is needed to download models, especially private ones, or to avoid rate limits.
HUGGINGFACE_TOKEN="hf_YOUR_TOKEN_HERE"

# --- Timeout & Logging Settings (optional) ---
# You can uncomment and change these if needed.
COMFYUI_STARTUP_TIMEOUT="120"
CELERY_TASK_TIME_LIMIT="180"
CELERY_TASK_AIOHTTP_TIMEOUT="200"
LOG_LEVEL="info"


---

__init__.py


---


# src/api.py (Добавляем новый эндпоинт /loras)

import logging
import os
from typing import Any, Dict, Optional, List

from fastapi import FastAPI, HTTPException
from fastapi.responses import FileResponse
from pydantic import BaseModel, HttpUrl

from .celery_app import celery_app
from .config import app_config
# --- ИЗМЕНЕНИЕ: Импортируем загрузчик манифестов ---
from .manifest_loader import validate_request, load_manifests
from .worker import generate_task

# ... (остальной код до get_task_status без изменений) ...
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)



from fastapi.concurrency import run_in_threadpool 

app = FastAPI(title="ComfyUI Production Service")

@app.get("/ping")
async def ping():
    """Простой эндпоинт для проверки, что сервер жив и видит изменения."""
    return {"message": "pong"}


class GenerationRequest(BaseModel):
    workflow_id: str
    params: Dict[str, Any] = {}
    callback_url: Optional[HttpUrl] = None
    
    
@app.post("/generate", status_code=202)
async def create_generation_task(request_data: GenerationRequest) -> Dict[str, str]:
    try:
        validated_params = validate_request(request_data.workflow_id, request_data.params)
    except ValueError as e:
        logger.error(f"Validation failed: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    callback_url_str = str(request_data.callback_url) if request_data.callback_url else None
    task = generate_task.delay(
        workflow_id=request_data.workflow_id, 
        params=validated_params,
        callback_url=callback_url_str
    )
    logger.info(f"Task {task.id} enqueued for workflow '{request_data.workflow_id}'.")
    return {"task_id": task.id}


@app.get("/loras", response_model=List[Dict[str, Any]])
async def list_available_loras():
    """
    Возвращает список LoRA, описанных в манифесте loras.yaml.
    Теперь манифест является единственным источником истины.
    """
    if not app_config.initialized:
        raise HTTPException(status_code=503, detail="Server is not yet initialized. Please try again in a moment.")

    lora_manifest = load_manifests().get("loras", {})
    
    if not lora_manifest:
        return []

    response_data = []
    # --- КЛЮЧЕВОЕ ИСПРАВЛЕНИЕ: Итерируемся по ключам манифеста ---
    for lora_filename, lora_info in lora_manifest.items():
        
        # Опциональная, но полезная проверка: существует ли такой файл на диске?
        if lora_filename not in app_config.AVAILABLE_LORAS:
            logger.warning(f"LoRA '{lora_filename}' is defined in loras.yaml but not found on disk. Skipping.")
            continue

        # --- Собираем данные только из lora_info, полученной из манифеста ---
        response_data.append({
            "name": lora_filename,
            "description": lora_info.get("description", "No description available."),
            "prefix": lora_info.get("prefix"),
            "suffix": lora_info.get("suffix"),
            "examples": lora_info.get("examples", [])
        })
        
    return response_data

# @app.get("/tasks/{task_id}")
# async def get_task_status(task_id: str) -> Dict[str, Any]:
#     # ... (этот эндпоинт без изменений) ...
#     task_result = celery_app.AsyncResult(task_id)
#     status = task_result.state 
#     response = {"task_id": task_id, "status": status}
#     if status == 'SUCCESS':
#         result_data = task_result.result
#         file_path_str = result_data.get('file_path')
#         if file_path_str:
#             base_url = app_config.PUBLIC_IP
#             file_name = os.path.basename(file_path_str)
#             download_url = f"{base_url}/results/{task_id}/{file_name}"
#             response["result"] = {"download_url": download_url}
#         else:
#             response["result"] = "Task succeeded but no file path was returned."
#     elif status == 'FAILURE':
#         response["result"] = str(task_result.info)
#     elif status == 'PROGRESS':
#         response["progress"] = task_result.info
#     elif status == 'PENDING':
#         response["result"] = "Task is waiting in the queue."
#     return response
@app.get("/tasks/{task_id}")
async def get_task_status(task_id: str) -> Dict[str, Any]:
    
    # --- НОВАЯ РЕАЛИЗАЦИЯ ---
    def check_celery_status():
        """Эта синхронная функция будет выполняться в отдельном потоке."""
        task_result = celery_app.AsyncResult(task_id)
        status = task_result.state
        
        response = {"task_id": task_id, "status": status}
        
        if status == 'SUCCESS':
            result_data = task_result.result
            file_path_str = result_data.get('file_path')
            if file_path_str:
                base_url = app_config.PUBLIC_IP
                file_name = os.path.basename(file_path_str)
                # --- ИСПРАВЛЕНИЕ: PUBLIC_IP должен быть полным URL с протоколом ---
                # Убедитесь, что в .env PUBLIC_IP = http://127.0.0.1:8000
                download_url = f"{base_url}/results/{task_id}/{file_name}"
                response["result"] = {"download_url": download_url}
            else:
                response["result"] = "Task succeeded but no file path was returned."
        elif status == 'FAILURE':
            response["result"] = str(task_result.info)
        elif status == 'PROGRESS':
            response["progress"] = task_result.info
        elif status == 'PENDING':
            response["result"] = "Task is waiting in the queue."
            
        return response

    # Выполняем блокирующую функцию в пуле потоков и ждем результат
    return await run_in_threadpool(check_celery_status)



@app.get("/results/{task_id}/{filename}")
async def download_result_file(task_id: str, filename: str):
    # ... (этот эндпоинт без изменений) ...
    task_result = celery_app.AsyncResult(task_id)
    if not task_result.ready() or task_result.status != 'SUCCESS':
        raise HTTPException(status_code=404, detail="Task not found or not completed successfully.")
    file_path = task_result.result.get('file_path')
    if not file_path:
        raise HTTPException(status_code=404, detail="File path not found in task result.")
    if os.path.basename(file_path) != filename:
        raise HTTPException(status_code=403, detail="Forbidden: Filename mismatch.")
    if os.path.exists(file_path):
        return FileResponse(path=file_path, media_type='image/png', filename=filename)
    else:
        logger.error(f"Result file not found on disk for task {task_id}: {file_path}")
        raise HTTPException(status_code=404, detail="Result file not found on disk.")



---


# src/celery_app.py

from celery import Celery

# Используем относительный импорт
from .config import app_config

celery_app = Celery(
    'comfy_tasks',
    broker=app_config.CELERY_BROKER_URL,
    backend=app_config.CELERY_BACKEND_URL,
    include=['src.worker']
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='Europe/Moscow',
    enable_utc=True,
)


---

# src/config.py (ФИНАЛЬНАЯ ВЕРСИЯ)

import os
import sys
from pathlib import Path
from typing import List
from dotenv import load_dotenv

# --- Load .env ---
project_root_for_env = Path(__file__).resolve().parent.parent
load_dotenv(dotenv_path=project_root_for_env / '.env')

# --- Path Configuration ---
# This ensures that ComfyUI's internal modules can be imported by our service.
project_root = Path(__file__).resolve().parent.parent
comfyui_path = project_root / "ComfyUI"
if str(comfyui_path) not in sys.path:
    sys.path.insert(0, str(comfyui_path))

import folder_paths

class AppConfig:
    """
    Singleton class for all application configuration.
    Reads settings from environment variables with sensible defaults.
    """
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(AppConfig, cls).__new__(cls)
            cls._instance.initialized = False
            
            # --- FastAPI & Network Settings ---
            # The host the Uvicorn server will bind to. '0.0.0.0' is necessary for
            # the service to be accessible from the host machine when running in WSL2/Docker.
            cls._instance.UVICORN_HOST = os.getenv("UVICORN_HOST", "0.0.0.0")
            cls._instance.UVICORN_PORT = int(os.getenv("UVICORN_PORT", 8000))

            # The public-facing IP used to generate correct download URLs.
            # For local WSL2 development, this should be '127.0.0.1' to be accessible
            # from the Windows host's browser. For LAN, it would be the machine's LAN IP.
            cls._instance.PUBLIC_IP = os.getenv("PUBLIC_IP", "127.0.0.1")

            # --- Celery & Redis Settings ---
            redis_password = os.getenv("REDIS_PASSWORD", "redis")
            redis_host = os.getenv("REDIS_HOST", "localhost")
            redis_port = os.getenv("REDIS_PORT", "6379")
            redis_db = os.getenv("REDIS_DB", "0")
            cls._instance.CELERY_BROKER_URL = f"redis://:{redis_password}@{redis_host}:{redis_port}/{redis_db}"
            cls._instance.CELERY_BACKEND_URL = cls._instance.CELERY_BROKER_URL

            # --- Timeout Settings (seconds) ---
            cls._instance.COMFYUI_STARTUP_TIMEOUT = int(os.getenv("COMFYUI_STARTUP_TIMEOUT", 120))
            cls._instance.CELERY_TASK_TIME_LIMIT = int(os.getenv("CELERY_TASK_TIME_LIMIT", 600))
            cls._instance.CELERY_TASK_AIOHTTP_TIMEOUT = int(os.getenv("CELERY_TASK_AIOHTTP_TIMEOUT", 300))

            # --- Logging Settings ---
            cls._instance.LOG_LEVEL = os.getenv("LOG_LEVEL", "info").lower()
            cls._instance.AVAILABLE_MODELS: List[str] = []
            cls._instance.AVAILABLE_LORAS: List[str] = []

        return cls._instance

    def initialize(self):
        """
        Performs one-time setup for ComfyUI paths and scans for available models.
        This must be called before the application starts accepting requests.
        """
        if self.initialized:
            return

        print("Initializing application configuration: setting and scanning model paths...")

        folder_paths.base_path = str(project_root)
        
        output_directory = comfyui_path / "output"
        input_directory = comfyui_path / "input"
        temp_directory = comfyui_path / "temp"
        
        output_directory.mkdir(exist_ok=True)
        input_directory.mkdir(exist_ok=True)
        temp_directory.mkdir(exist_ok=True)

        folder_paths.set_output_directory(str(output_directory))
        folder_paths.set_input_directory(str(input_directory))
        folder_paths.set_temp_directory(str(temp_directory))

        
        # Scan for models using a whitelist of valid file extensions.
        VALID_MODEL_EXTENSIONS = {".safetensors", ".ckpt", ".pt", ".pth", ".bin", ".gguf"}
        model_dirs_to_scan = [comfyui_path / "models" / "checkpoints", comfyui_path / "models" / "unet"]
        
        all_models = set()
        for model_dir in model_dirs_to_scan:
            if not model_dir.is_dir():
                continue
            for filepath in model_dir.rglob('*'):
                # Условие: это файл, его расширение есть в нашем белом списке, и это не системный файл (не начинается с точки)
                if filepath.is_file() and filepath.suffix.lower() in VALID_MODEL_EXTENSIONS and not filepath.name.startswith('.'):
                    all_models.add(filepath.name)
        
        self.AVAILABLE_MODELS = sorted(list(all_models))

        # Scan for LoRAs using ComfyUI's built-in function, which handles extensions correctly.
        lora_dirs = ["loras"]
        all_loras = set()
        for dir_type in lora_dirs:
            try:
                # get_filename_list уже корректно фильтрует по расширениям
                all_loras.update(folder_paths.get_filename_list(dir_type))
            except Exception as e:
                print(f"Warning: Could not read LoRAs from '{dir_type}' directory: {e}")

        if "None" not in all_loras:
            all_loras.add("None")
        self.AVAILABLE_LORAS = sorted(list(all_loras))
        
        print(f"Scan complete. Found {len(self.AVAILABLE_MODELS)} models and {len(self.AVAILABLE_LORAS)} LoRAs.")
        if not self.AVAILABLE_MODELS:
            print("CRITICAL WARNING: No models found. Check your model directories.")
        else:
            print(f"Available models found: {self.AVAILABLE_MODELS}")

        self.initialized = True

# Global singleton instance for easy access across the application.
app_config = AppConfig()



---



# src/main.py

import uvicorn

from .config import app_config
from .api import app

def main():
    """
    Главная функция для запуска приложения.
    Инициализирует конфигурацию и стартует Uvicorn.
    """
    print("--- ComfyUI Production Service ---")
    
    app_config.initialize()
    
    print(f"\nStarting FastAPI server with Uvicorn on {app_config.UVICORN_HOST}:{app_config.UVICORN_PORT}...")
    uvicorn.run(
        app, 
        host=app_config.UVICORN_HOST, 
        port=app_config.UVICORN_PORT, 
        log_level=app_config.LOG_LEVEL
    )

if __name__ == "__main__":
    main()




---


# src/manifest_loader.py (Реализует обе задачи)

import yaml
from pathlib import Path
from functools import lru_cache
import random
from typing import Dict, Any

from .config import app_config

EXPERIMENTAL_WORKFLOW_PREFIX = "exp_"
MANIFEST_DIR = Path(__file__).parent / "manifests"

@lru_cache(maxsize=1)
def load_manifests():
    """Загружает и кеширует все YAML-манифесты."""
    with open(MANIFEST_DIR / "base.yaml", 'r') as f:
        base_manifest = yaml.safe_load(f)
    with open(MANIFEST_DIR / "workflows.yaml", 'r') as f:
        workflows_manifest = yaml.safe_load(f)
    
    loras_manifest_path = MANIFEST_DIR / "loras.yaml"
    loras_manifest = {}
    if loras_manifest_path.is_file():
        with open(loras_manifest_path, 'r') as f:
            loras_manifest = yaml.safe_load(f)

    return {
        "base": base_manifest,
        "workflows": workflows_manifest,
        "loras": loras_manifest
    }


def apply_lora_prompt_modifiers(params: Dict[str, Any], lora_manifest: Dict[str, Any]) -> Dict[str, Any]:
    """
    Проверяет выбранную LoRA и модифицирует промпт, если для нее
    в манифесте указаны префикс и/или суффикс.
    Автоматически добавляет запятые и пробелы.
    """
    lora_name = params.get("lora")
    original_prompt = params.get("prompt")

    if not lora_name or lora_name.lower() == "none" or not original_prompt:
        return params

    lora_info = lora_manifest.get(lora_name)
    if not lora_info:
        return params

    # --- ИСПРАВЛЕННАЯ ЛОГИКА СБОРКИ ПРОМПТА ---
    prompt_parts = [original_prompt.strip()]
    
    prefix = lora_info.get("prefix")
    if prefix:
        # Вставляем префикс в начало списка
        prompt_parts.insert(0, prefix.strip())

    suffix = lora_info.get("suffix")
    if suffix:
        # Добавляем суффикс в конец списка
        prompt_parts.append(suffix.strip())
    
    # Собираем финальный промпт, соединяя все части через ", "
    # Это сработает корректно для 1, 2 или 3 частей.
    modified_prompt = ", ".join(filter(None, prompt_parts))
    
    if modified_prompt != original_prompt:
        print(f"[INFO] Prompt modified by LoRA '{lora_name}': \"{modified_prompt[:80]}...\"")
        params["prompt"] = modified_prompt

    return params


def validate_request(workflow_id: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Валидирует запрос, поддерживает exp_* workflow и применяет модификаторы LoRA.
    """
    if not app_config.initialized:
        raise RuntimeError("Application config not initialized. Run the app via main.py.")

    manifests = load_manifests()
    base_params_info = manifests["base"]
    workflows = manifests["workflows"]
    
    # ... (логика для exp_* workflow остается без изменений) ...
    allowed_params = set()
    is_experimental = workflow_id.startswith(EXPERIMENTAL_WORKFLOW_PREFIX)
    if is_experimental:
        workflow_file_path = Path(__file__).parent / "workflows" / f"{workflow_id}.json"
        if not workflow_file_path.is_file():
            raise ValueError(f"Experimental workflow file '{workflow_id}.json' not found in 'src/workflows/'.")
        allowed_params = set(base_params_info.keys())
        is_required_map = {}
    else:
        if workflow_id not in workflows:
            raise ValueError(f"Workflow '{workflow_id}' not found in manifest.")
        workflow_info = workflows[workflow_id]
        allowed_params = set(workflow_info.get("parameters", []))
        is_required_map = {p: True for p, o in workflow_info.get("overrides", {}).items() if o.get("required")}

    validated_params = {}

    # Сначала валидируем и собираем все параметры как обычно
    for param_name in allowed_params:
        if param_name not in base_params_info:
            continue
        param_info = base_params_info[param_name]
        is_required = is_required_map.get(param_name, False)
        value = params.get(param_name)
        if is_required and value is None:
            raise ValueError(f"Missing required parameter: '{param_name}'")
        if value is None:
            value = param_info.get("default")
        if param_name == "seed" and str(value).lower() == "random":
            value = random.SystemRandom().randint(0, 2**63 - 1)
        param_type = param_info.get("type")
        try:
            if value is not None:
                if param_type == "string": value = str(value)
                elif param_type == "integer": value = int(value)
                elif param_type == "float": value = float(value)
                elif param_type == "boolean": value = bool(value)
        except (TypeError, ValueError):
            raise ValueError(f"Parameter '{param_name}' with value '{value}' must be of type {param_type}.")
        if param_name == "model" and value not in app_config.AVAILABLE_MODELS:
            raise ValueError(f"Model '{value}' not found.")
        if param_name == "lora" and value not in app_config.AVAILABLE_LORAS:
            raise ValueError(f"LoRA '{value}' not found.")
        
        map_to_key = param_info.get("map_to", param_name)
        validated_params[map_to_key] = value
    
    # --- НОВЫЙ ШАГ: Применяем модификаторы LoRA ---
    # Эта функция будет изменять validated_params "по месту"
    validated_params = apply_lora_prompt_modifiers(validated_params, manifests["loras"])
        
    return validated_params



---


# src/worker.py (ВЕРСИЯ ДЛЯ ОТЛАДКИ ПРОГРЕССА)

import os, sys, logging, json, uuid, aiohttp, asyncio, subprocess, time, urllib.request, urllib.error
from pathlib import Path
from typing import Dict, Any, Optional
from celery.signals import worker_process_init
from celery.app.task import Task
import fcntl

from .config import app_config
from .celery_app import celery_app
from .workflow_utils import populate_workflow

# --- Код до execute_workflow_async без изменений ---
project_root = Path(__file__).resolve().parent.parent
COMFYUI_ROOT = project_root / "ComfyUI"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
comfy_server_instance: Optional[subprocess.Popen] = None
comfy_server_url: Optional[str] = None
comfy_output_dir: Optional[Path] = None

def set_pipe_size():
    # Эта функция будет выполняться уже ПОСЛЕ fork(), но ПЕРЕД exec()
    # в новом, дочернем процессе. Это важно.
    try:
        # F_SETPIPE_SZ доступен в Linux начиная с ядра 2.6.35
        # 134217728 байт = 128 МБ
        # Мы устанавливаем размер буфера для stdout (файловый дескриптор 1)
        fcntl.fcntl(1, fcntl.F_SETPIPE_SZ, 134217728)
        # И для stderr (файловый дескриптор 2)
        fcntl.fcntl(2, fcntl.F_SETPIPE_SZ, 134217728)
    except (IOError, AttributeError, NameError) as e:
        # Если fcntl.F_SETPIPE_SZ не поддерживается, ничего страшного.
        # Просто логируем это. В большинстве современных дистрибутивов он есть.
        # Используем os.write, так как стандартный logging может быть еще не настроен.
        import os
        os.write(2, f"Could not set pipe size: {e}\n".encode())
    
    # Также вызываем вашу оригинальную функцию для управления группами процессов
    if os.name == 'posix':
        os.setpgrp()


def ensure_comfy_server_is_running():
    global comfy_server_instance, comfy_server_url, comfy_output_dir
    if comfy_server_instance and comfy_server_instance.poll() is None: return
    if comfy_server_instance: logger.warning(f"ComfyUI process died with code {comfy_server_instance.poll()}. Restarting...")
    logger.info("Starting a fresh ComfyUI server instance on a random port...")
    import socket
    
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0)); port = s.getsockname()[1]
    comfy_output_dir = COMFYUI_ROOT / "output"
    comfy_output_dir.mkdir(exist_ok=True)
    command = [sys.executable, "main.py", "--port", str(port), "--output-directory", str(comfy_output_dir), "--preview-method", "none", "--dont-print-server", "--disable-auto-launch"]
    proc = subprocess.Popen(command, cwd=str(COMFYUI_ROOT), preexec_fn=set_pipe_size)
    url_to_check = f"http://127.0.0.1:{port}/object_info"
    for _ in range(app_config.COMFYUI_STARTUP_TIMEOUT):
        if proc.poll() is not None: raise RuntimeError(f"ComfyUI process terminated unexpectedly during startup.")
        try:
            with urllib.request.urlopen(url_to_check, timeout=1) as response:
                if response.status == 200:
                    logger.info(f"ComfyUI server is ready on port {port}.")
                    comfy_server_instance, comfy_server_url = proc, f"http://127.0.0.1:{port}"
                    return
        except Exception: time.sleep(1)
    proc.terminate(); proc.wait()
    raise RuntimeError(f"ComfyUI server failed to start on port {port}.")

@worker_process_init.connect
def on_worker_start(**kwargs):
    logger.info("Worker process started. Pre-warming ComfyUI server...")
    try: ensure_comfy_server_is_running()
    except Exception as e: logger.critical(f"FATAL: Failed to start ComfyUI on worker init: {e}", exc_info=True)

async def send_callback(url: str, data: Dict[str, Any]):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=data) as response:
                response.raise_for_status()
                logger.info(f"Successfully sent callback to {url}")
    except Exception as e: logger.error(f"Exception occurred while sending callback to {url}: {e}", exc_info=True)

async def execute_workflow_async(task: Task, populated_workflow: Dict[str, Any]) -> str:
    task_id = task.request.id
    client_id = str(uuid.uuid4())
    http_server_address = comfy_server_url.replace("http://", "")
    ws_server_address = f"ws://{http_server_address}/ws?clientId={client_id}"

    async with aiohttp.ClientSession() as session:
        prompt_data = {'prompt': populated_workflow, "client_id": client_id}
        async with session.post(f"{comfy_server_url}/prompt", json=prompt_data) as response:
            response.raise_for_status()
            result = await response.json()
            prompt_id = result.get("prompt_id")
            if not prompt_id: raise ValueError("API call to /prompt did not return a prompt_id.")
            logger.info(f"[{task_id}] Workflow queued with prompt_id: {prompt_id}")

        async with session.ws_connect(ws_server_address, timeout=app_config.CELERY_TASK_AIOHTTP_TIMEOUT) as ws:
            logger.info(f"[{task_id}] WebSocket connected.")
            task.update_state(state='PROGRESS', meta={'step_name': 'Queued', 'eta_seconds': -1, 'percent': 0, 'current': 0, 'total': 1})

            execution_complete = False
            
            async for msg in ws:
                if isinstance(msg.data, str):
                    message = json.loads(msg.data)
                    msg_data = message.get('data', {})
                    
                    if 'prompt_id' in msg_data and msg_data['prompt_id'] == prompt_id:
                        
                        if message['type'] == 'progress':
                            current_step = msg_data['value']
                            total_steps = msg_data['max']
                               
                            task.update_state(
                                state='PROGRESS',
                                meta={
                                    'current': current_step,
                                    'total': total_steps,
                                    'percent': round((current_step / total_steps) * 100, 2),
                                    'step_name': 'Generating',
                                }
                            )
                        
                        elif message['type'] == 'executing':
                            if msg_data.get('node') is None:
                                logger.info(f"[{task_id}] Received completion signal.")
                                execution_complete = True
                                break

            if not execution_complete:
                raise TimeoutError(f"WebSocket connection closed before the completion signal was received for prompt {prompt_id}.")

        logger.info(f"[{task_id}] Retrieving output from /history/{prompt_id}")
        await asyncio.sleep(0.5)
        async with session.get(f"{comfy_server_url}/history/{prompt_id}") as history_resp:
            history_resp.raise_for_status()
            history = await history_resp.json()
            
            if prompt_id in history:
                prompt_history = history[prompt_id]
                for _, node_output in prompt_history.get('outputs', {}).items():
                    if "images" in node_output and node_output["images"]:
                        image = node_output["images"][0]
                        return str(comfy_output_dir / image.get("subfolder", "") / image["filename"])
            
            logger.error(f"[{task_id}] Critical: Output not found in history. History dump: {json.dumps(history)}")
            raise FileNotFoundError("Could not find output file in ComfyUI's history after execution.")
        
# --- Основная задача Celery остается без изменений ---
@celery_app.task(name="generate_task", bind=True, acks_late=True, time_limit=app_config.CELERY_TASK_TIME_LIMIT)
def generate_task(self: Task, workflow_id: str, params: Dict[str, Any], callback_url: Optional[str] = None) -> Dict[str, Any]:
    task_id = self.request.id
    try:
        ensure_comfy_server_is_running()
        workflow_path = project_root / "src" / "workflows" / f"{workflow_id}.json"
        with open(workflow_path, "r") as f: workflow_data = json.load(f)
        populated_workflow = populate_workflow(workflow_data, params)
        file_path = asyncio.run(execute_workflow_async(self, populated_workflow))
        if callback_url:
            base_url = app_config.PUBLIC_IP
            file_name = os.path.basename(file_path)
            download_url = f"{base_url}/results/{task_id}/{file_name}"
            callback_data = {"task_id": task_id, "status": "SUCCESS", "result": {"download_url": download_url}}
            asyncio.run(send_callback(callback_url, callback_data))
        return {"file_path": file_path}
    except Exception as e:
        logger.error(f"Task {task_id} failed: {e}", exc_info=True)
        if callback_url:
            callback_data = {"task_id": task_id, "status": "FAILURE", "result": str(e)}
            asyncio.run(send_callback(callback_url, callback_data))
        raise



---


# src/workflow_utils.py

import copy
import logging
from typing import Any, Dict

logger = logging.getLogger(__name__)

def populate_workflow(workflow_data: Dict[str, Any], api_params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Вставляет параметры из API в JSON-объект workflow.
    Это легковесная, контролируемая и независимая функция.

    Она ищет узлы в `workflow_data` по их заголовку (`_meta.title`)
    и заменяет значение в `inputs.value` на значение из `api_params`.

    Args:
        workflow_data: Исходный JSON воркфлоу, загруженный из файла.
        api_params: Словарь с параметрами, пришедшими из API-запроса.
                    Ключи этого словаря должны совпадать с `_meta.title`
                    входных узлов в воркфлоу.

    Returns:
        Новый словарь воркфлоу с подставленными значениями.
    """
    workflow = copy.deepcopy(workflow_data)

    title_to_node_id_map: Dict[str, str] = {}
    for node_id, node_info in workflow.items():
        title = node_info.get("_meta", {}).get("title")
        if title:
            title_to_node_id_map[title] = node_id

    for param_name, param_value in api_params.items():
        if param_name in title_to_node_id_map:
            target_node_id = title_to_node_id_map[param_name]
            
            if 'inputs' in workflow[target_node_id] and 'value' in workflow[target_node_id]['inputs']:
                workflow[target_node_id]['inputs']['value'] = param_value
                logger.debug(f"Populated node '{target_node_id}' (title: '{param_name}') with value: {param_value}")
            else:
                logger.warning(f"Could not set parameter '{param_name}'. Node '{target_node_id}' has no 'inputs.value' field.")
        else:
            logger.warning(f"Parameter '{param_name}' from API request has no corresponding input node with that title in the workflow.")

    return workflow


---


# service/manifests/base.yaml

# --- Стандартные параметры изображения ---
width:
  map_to: "width"
  type: "integer"
  default: 1024
height:
  map_to: "height"
  type: "integer"
  default: 1024

# --- Параметры генерации ---
prompt:
  map_to: "prompt"
  type: "string"
seed:
  map_to: "seed"
  type: "integer"
  default: "random"
  special_handlers:
    - value: "random"
      action: "generate_random_int"
      args: { min: 0, max: 18446744073709551615 }
steps:
  map_to: "steps"
  type: "integer"
  default: 20

# --- Параметры LoRA ---
lora:
  map_to: "lora"
  type: "string"
  default: "None"
lora_strength:
  map_to: "lora_strength"
  type: "float"
  default: 1.0

# --- Параметры модели и оптимизации ---
model:
  map_to: "model"
  type: "string"
  default: "flux1-schnell-Q4_K_S.gguf"
FBC_optimize:
  map_to: "FBC_optimize"
  type: "boolean"
  default: true

# --- Параметры для будущих I2I workflows ---
input_image:
  map_to: "input_image"
  type: "file" # Специальный тип для файлов
denoising_strength:
  map_to: "denoising_strength"
  type: "float"
  default: 0.8


---


# service/manifests/workflows.yaml

flux_default:
  workflow_file: "flux_default.json"
  description: "Optimized Flux T2I with full API control."
  parameters:
    - prompt # Использует параметр "prompt" из base.yaml
    - seed
    - width
    - height
    - steps
    - lora
    - lora_strength
    - model
    - FBC_optimize

  # Здесь мы можем переопределить свойства для этого конкретного workflow
  overrides:
    prompt:
      required: true # Для этого workflow промпт обязателен

# Пример для будущего
# sdxl_upscale:
#  workflow_file: "sdxl_upscale.json"
#  description: "Upscales an image using SDXL."
#  parameters:
#    - input_image
#  overrides:
#    input_image:
#      required: true


---

# src/manifests/loras.yaml


"Flux-Ghibli-Art-LoRA.safetensors":

  prefix: "Ghibli Art"

  description: "A beautiful anime style inspired by Studio Ghibli, featuring lush, painterly landscapes and iconic puffy clouds"

  examples:
    - "assets/lora_examples/Ghibli-Art/example_1.png"
    - "assets/lora_examples/Ghibli-Art/example_2.png"
    - "assets/lora_examples/Ghibli-Art/example_3.png"


"Flux-Scenery-LoRA.safetensors":

  suffix: "scenery style"

  description: "An epic fantasy concept art style, characterized by painterly textures, intricate architecture, and dramatic, atmospheric lighting"


"Flux-Anime-LoRA.safetensors":

  prefix: "an anm"

  description: "A scenic anime aesthetic featuring rich, emotional color palettes, detailed natural backgrounds, and a clean, polished character art style"


"Flux-Dev-Real-Anime-LoRA.safetensors":

  prefix: "Real Anime"

  #suffix: "Fashion photography, high resolution, 50mm lens, f/2.8, natural lighting, global illumination. --ar 85:128 --v 6.0 --style raw."

  description: "A high-fidelity anime style defined by cinematic lighting, soft painterly shading, and clean linework for a polished feel."


"Flux-Ghibsky-Art-LoRA.safetensors":

  prefix: "GHIBSKY style"

  description: "Fusion of styles creates enchanting scenes that capture the essence of both Ghibli's whimsical charm and Makoto Shinkai's atmospheric beauty"
  # Use GHIBSKY style to invoke the model’s unique aesthetic. It’s best to start your prompt with the trigger word,
  # followed by descriptions of your scene, such as nature, skies, houses, roads, villages, etc.
  # If you are getting too realistic images, try adding painting to your prompt, for example: GHIBSKY style painting.

---


#src/workflows/flux_default.json
{
  "6": {
    "inputs": {
      "text": [
        "67",
        0
      ],
      "clip": [
        "41",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Positive Prompt)"
    }
  },
  "8": {
    "inputs": {
      "samples": [
        "13",
        0
      ],
      "vae": [
        "10",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "10": {
    "inputs": {
      "vae_name": "ae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "13": {
    "inputs": {
      "noise": [
        "25",
        0
      ],
      "guider": [
        "22",
        0
      ],
      "sampler": [
        "16",
        0
      ],
      "sigmas": [
        "17",
        0
      ],
      "latent_image": [
        "27",
        0
      ]
    },
    "class_type": "SamplerCustomAdvanced",
    "_meta": {
      "title": "SamplerCustomAdvanced"
    }
  },
  "16": {
    "inputs": {
      "sampler_name": "euler"
    },
    "class_type": "KSamplerSelect",
    "_meta": {
      "title": "KSamplerSelect"
    }
  },
  "17": {
    "inputs": {
      "scheduler": "simple",
      "steps": [
        "65",
        0
      ],
      "denoise": 1,
      "model": [
        "46",
        0
      ]
    },
    "class_type": "BasicScheduler",
    "_meta": {
      "title": "BasicScheduler"
    }
  },
  "22": {
    "inputs": {
      "model": [
        "30",
        0
      ],
      "conditioning": [
        "26",
        0
      ]
    },
    "class_type": "BasicGuider",
    "_meta": {
      "title": "BasicGuider"
    }
  },
  "25": {
    "inputs": {
      "noise_seed": [
        "66",
        0
      ]
    },
    "class_type": "RandomNoise",
    "_meta": {
      "title": "RandomNoise"
    }
  },
  "26": {
    "inputs": {
      "guidance": 3.5,
      "conditioning": [
        "6",
        0
      ]
    },
    "class_type": "FluxGuidance",
    "_meta": {
      "title": "FluxGuidance"
    }
  },
  "27": {
    "inputs": {
      "width": [
        "63",
        0
      ],
      "height": [
        "64",
        0
      ],
      "batch_size": 1
    },
    "class_type": "EmptySD3LatentImage",
    "_meta": {
      "title": "EmptySD3LatentImage"
    }
  },
  "30": {
    "inputs": {
      "max_shift": 1.15,
      "base_shift": 0.5,
      "width": [
        "63",
        0
      ],
      "height": [
        "64",
        0
      ],
      "model": [
        "46",
        0
      ]
    },
    "class_type": "ModelSamplingFlux",
    "_meta": {
      "title": "ModelSamplingFlux"
    }
  },
  "38": {
    "inputs": {
      "object_to_patch": "diffusion_model",
      "residual_diff_threshold": 0.12,
      "start": 0,
      "end": 1,
      "max_consecutive_cache_hits": -1,
      "model": [
        "40",
        0
      ]
    },
    "class_type": "ApplyFBCacheOnModel",
    "_meta": {
      "title": "Apply First Block Cache"
    }
  },
  "39": {
    "inputs": {
      "is_patcher": true,
      "object_to_patch": "diffusion_model",
      "compiler": "torch.compile",
      "fullgraph": false,
      "dynamic": false,
      "mode": "",
      "options": "",
      "disable": false,
      "backend": "inductor"
    },
    "class_type": "EnhancedCompileModel",
    "_meta": {
      "title": "Compile Model+"
    }
  },
  "40": {
    "inputs": {
      "unet_name": [
        "73",
        0
      ]
    },
    "class_type": "UnetLoaderGGUF",
    "_meta": {
      "title": "Unet Loader (GGUF)"
    }
  },
  "41": {
    "inputs": {
      "clip_name1": "t5-v1_1-xxl-encoder-Q5_K_M.gguf",
      "clip_name2": "clip_l.safetensors",
      "type": "flux"
    },
    "class_type": "DualCLIPLoaderGGUF",
    "_meta": {
      "title": "DualCLIPLoader (GGUF)"
    }
  },
  "46": {
    "inputs": {
      "lora_stack": [
        "47",
        0
      ],
      "model": [
        "60",
        0
      ],
      "optional_clip": [
        "41",
        0
      ]
    },
    "class_type": "easy loraStackApply",
    "_meta": {
      "title": "Easy Apply LoraStack"
    }
  },
  "47": {
    "inputs": {
      "toggle": true,
      "mode": "simple",
      "num_loras": 1,
      "lora_1_name": [
        "74",
        0
      ],
      "lora_1_strength": [
        "75",
        0
      ],
      "lora_1_model_strength": 1.0,
      "lora_1_clip_strength": 1.0,
      "lora_2_name": "None",
      "lora_2_strength": 1.0,
      "lora_2_model_strength": 1.0,
      "lora_2_clip_strength": 1.0,
      "lora_3_name": "None",
      "lora_3_strength": 1.0,
      "lora_3_model_strength": 1.0,
      "lora_3_clip_strength": 1.0,
      "lora_4_name": "None",
      "lora_4_strength": 1.0,
      "lora_4_model_strength": 1.0,
      "lora_4_clip_strength": 1.0,
      "lora_5_name": "None",
      "lora_5_strength": 1.0,
      "lora_5_model_strength": 1.0,
      "lora_5_clip_strength": 1.0,
      "lora_6_name": "None",
      "lora_6_strength": 1.0,
      "lora_6_model_strength": 1.0,
      "lora_6_clip_strength": 1.0,
      "lora_7_name": "None",
      "lora_7_strength": 1.0,
      "lora_7_model_strength": 1.0,
      "lora_7_clip_strength": 1.0,
      "lora_8_name": "None",
      "lora_8_strength": 1.0,
      "lora_8_model_strength": 1.0,
      "lora_8_clip_strength": 1.0,
      "lora_9_name": "None",
      "lora_9_strength": 1.0,
      "lora_9_model_strength": 1.0,
      "lora_9_clip_strength": 1.0,
      "lora_10_name": "None",
      "lora_10_strength": 1.0,
      "lora_10_model_strength": 1.0,
      "lora_10_clip_strength": 1.0
    },
    "class_type": "easy loraStack",
    "_meta": {
      "title": "EasyLoraStack"
    }
  },
  "60": {
    "inputs": {
      "model_a": [
        "40",
        0
      ],
      "model_b": [
        "38",
        0
      ],
      "select_b": [
        "72",
        0
      ]
    },
    "class_type": "HelperModelSwitch",
    "_meta": {
      "title": "Model Switch"
    }
  },
  "63": {
    "inputs": {
      "value": 1024
    },
    "class_type": "ParamInt",
    "_meta": {
      "title": "width"
    }
  },
  "64": {
    "inputs": {
      "value": 1024
    },
    "class_type": "ParamInt",
    "_meta": {
      "title": "height"
    }
  },
  "65": {
    "inputs": {
      "value": 4
    },
    "class_type": "ParamInt",
    "_meta": {
      "title": "steps"
    }
  },
  "66": {
    "inputs": {
      "value": 0
    },
    "class_type": "ParamInt",
    "_meta": {
      "title": "seed"
    }
  },
  "67": {
    "inputs": {
      "value": "wild nature"
    },
    "class_type": "ParamString",
    "_meta": {
      "title": "prompt"
    }
  },
  "72": {
    "inputs": {
      "value": true
    },
    "class_type": "ParamBoolean",
    "_meta": {
      "title": "FBC_optimize"
    }
  },
  "73": {
    "inputs": {
      "value": "flux1-schnell-Q4_K_S.gguf"
    },
    "class_type": "ParamUniversal",
    "_meta": {
      "title": "model"
    }
  },
  "74": {
    "inputs": {
      "value": "None"
    },
    "class_type": "ParamUniversal",
    "_meta": {
      "title": "lora"
    }
  },
  "75": {
    "inputs": {
      "value": 0.85
    },
    "class_type": "ParamFloat",
    "_meta": {
      "title": "lora_strength"
    }
  },
  "76": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "8",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  }
}


---

#src/install/requirements.lock.txt
a2wsgi==1.10.8
accelerate==1.7.0
aiohappyeyeballs==2.6.1
aiohttp==3.12.12
aiosignal==1.3.2
aiosqlite==0.21.0
albucore==0.0.24
albumentations==2.0.8
amqp==5.3.1
annotated-types==0.7.0
anyio==4.9.0
appdirs==1.4.4
arrow==1.3.0
asgiref==3.8.1
attrs==25.3.0
av==14.4.0
beautifulsoup4==4.13.4
billiard==4.2.1
binaryornot==0.4.4
blinker==1.9.0
cattrs==23.1.2
celery==5.5.3
certifi==2025.4.26
cffi==1.17.1
chardet==5.2.0
charset-normalizer==3.4.2
click==8.1.8
click-didyoumean==0.3.1
click-option-group==0.5.7
click-plugins==1.1.1
click-repl==0.3.0
clip-interrogator==0.6.0
cloudpickle==3.1.1
coloredlogs==15.0.1
colour-science==0.4.6
comfy-cli==1.4.1
comfyui-embedded-docs==0.2.0
comfyui_frontend_package==1.21.7
comfyui_workflow_templates==0.1.25
contourpy==1.3.2
cookiecutter==2.6.0
cycler==0.12.1
diffusers==0.33.1
dill==0.4.0
duckduckgo_search==8.0.4
easydict==1.13
einops==0.8.1
fastapi==0.115.12
filelock==3.18.0
Flask==3.1.1
flatbuffers==25.2.10
fonttools==4.58.3
frozenlist==1.7.0
fs==2.4.16
fsspec==2025.5.1
ftfy==6.3.1
gdown==5.2.0
gevent==25.5.1
gguf==0.17.0
gitdb==4.0.12
GitPython==3.1.44
greenlet==3.2.3
h11==0.16.0
hf-xet==1.1.3
hiredis==3.2.1
httpcore==1.0.9
httptools==0.6.4
httpx==0.28.1
httpx-ws==0.7.2
huggingface-hub==0.33.0
humanfriendly==10.0
idna==3.10
imageio==2.37.0
importlib_metadata==8.7.0
itsdangerous==2.2.0
Jinja2==3.1.6
jsonschema==4.24.0
jsonschema-specifications==2025.4.1
kantoku==0.18.3
kiwisolver==1.4.8
kombu==5.5.4
kornia==0.8.1
kornia_rs==0.1.9
lark==1.2.2
lazy_loader==0.4
llvmlite==0.44.0
lxml==5.4.0
markdown-it-py==3.0.0
MarkupSafe==3.0.2
matplotlib==3.10.3
mdurl==0.1.2
mixpanel==4.10.1
mpmath==1.3.0
multidict==6.4.4
networkx==3.5
numba==0.61.2
numpy==2.2.6
nvidia-cublas-cu12==12.6.4.1
nvidia-cuda-cupti-cu12==12.6.80
nvidia-cuda-nvrtc-cu12==12.6.77
nvidia-cuda-runtime-cu12==12.6.77
nvidia-cudnn-cu12==9.5.1.17
nvidia-cufft-cu12==11.3.0.4
nvidia-cufile-cu12==1.11.1.6
nvidia-curand-cu12==10.3.7.77
nvidia-cusolver-cu12==11.7.1.2
nvidia-cusparse-cu12==12.5.4.2
nvidia-cusparselt-cu12==0.6.3
nvidia-ml-py==12.575.51
nvidia-nccl-cu12==2.26.2
nvidia-nvjitlink-cu12==12.6.85
nvidia-nvtx-cu12==12.6.77
onnxruntime==1.22.0
open_clip_torch==2.32.0
opencv-python==4.11.0.86
opencv-python-headless==4.11.0.86
opentelemetry-api==1.34.1
opentelemetry-instrumentation==0.55b1
opentelemetry-instrumentation-aiohttp-client==0.55b1
opentelemetry-instrumentation-asgi==0.55b1
opentelemetry-sdk==1.34.1
opentelemetry-semantic-conventions==0.55b1
opentelemetry-util-http==0.55b1
packaging==25.0
pathspec==0.12.1
peft==0.15.2
piexif==1.1.3
pillow==11.2.1
pip-requirements-parser==32.0.1
pixeloe==0.1.4
platformdirs==4.3.8
pooch==1.8.2
primp==0.15.0
prometheus_client==0.22.1
prompt_toolkit==3.0.51
propcache==0.3.2
protobuf==6.31.1
psutil==7.0.0
pycparser==2.22
pydantic==2.11.6
pydantic_core==2.33.2
Pygments==2.19.1
PyMatting==1.1.14
pyparsing==3.2.3
PySocks==1.7.1
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-json-logger==3.3.0
python-multipart==0.0.20
python-slugify==8.0.4
PyYAML==6.0.2
pyzmq==26.4.0
questionary==2.1.0
redis==6.2.0
referencing==0.36.2
regex==2024.11.6
rembg==2.0.66
requests==2.32.4
rich==14.0.0
rpds-py==0.25.1
ruff==0.11.13
safetensors==0.5.3
schema==0.7.7
scikit-image==0.25.2
scipy==1.15.3
segment-anything==1.0
semver==3.0.4
sentencepiece==0.2.0
shellingham==1.5.4
simple-di==0.1.5
simsimd==6.4.9
six==1.17.0
smmap==5.0.2
sniffio==1.3.1
soundfile==0.13.1
soupsieve==2.7
spandrel==0.4.1
starlette==0.46.2
stringzilla==3.12.5
sympy==1.14.0
text-unidecode==1.3
tifffile==2025.6.11
timm==1.0.15
tokenizers==0.21.1
tomli_w==1.2.0
tomlkit==0.13.3
torch==2.7.0
torchaudio==2.7.0
torchsde==0.2.6
torchvision==0.22.0
tornado==6.5.1
tqdm==4.67.1
trampoline==0.1.2
transformers==4.52.4
transparent-background==1.3.4
triton==3.3.0
typer==0.16.0
types-python-dateutil==2.9.0.20250516
typing-inspection==0.4.1
typing_extensions==4.14.0
tzdata==2025.2
urllib3==2.4.0
uv==0.7.13
uvicorn==0.34.3
uvloop==0.21.0
vine==5.1.0
watchfiles==1.0.5
wcwidth==0.2.13
websocket-client==1.8.0
websockets==15.0.1
Werkzeug==3.1.3
wget==3.2
wrapt==1.17.2
wsproto==1.2.0
xformers==0.0.30
yarl==1.20.1
zipp==3.23.0
zope.event==5.1
zope.interface==7.2


---


#src/install/install.sh

#!/bin/bash
# Exit immediately if a command exits with a non-zero status.
set -e

# --- Configuration ---
# Get the absolute path of the project's root directory
PROJECT_ROOT="$( cd "$( dirname "${BASH_SOURCE[0]}" )/.." && pwd )"
VENV_DIR="$PROJECT_ROOT/venv"
COMFYUI_DIR="$PROJECT_ROOT/ComfyUI"
PYTHON_EXEC="$VENV_DIR/bin/python3"
PIP_EXEC="$VENV_DIR/bin/pip"
REQUIREMENTS_FILE="$PROJECT_ROOT/install/requirements.lock.txt"
INSTALL_SCRIPTS_DIR="$PROJECT_ROOT/install/scripts"

# --- Helper Functions ---
check_dependencies() {
    echo "--> Checking for required system dependencies..."
    for cmd in python3 git; do
        if ! command -v "$cmd" &> /dev/null; then
            echo "ERROR: Command '$cmd' not found. Please install it."
            exit 1
        fi
    done
    echo "All dependencies found."
}

setup_venv() {
    echo "--> Setting up Python virtual environment..."
    if [ ! -d "$VENV_DIR" ]; then
        echo "Creating virtual environment at $VENV_DIR"
        python3 -m venv "$VENV_DIR"
    else
        echo "Virtual environment already exists."
    fi

    echo "Installing/updating dependencies from $REQUIREMENTS_FILE"
    "$PIP_EXEC" install -U pip
    "$PIP_EXEC" install -r "$REQUIREMENTS_FILE"
}

install_comfyui() {
    echo "--> Setting up ComfyUI..."
    if [ ! -d "$COMFYUI_DIR" ]; then
        echo "Cloning ComfyUI repository..."
        git clone https://github.com/comfyanonymous/ComfyUI.git "$COMFYUI_DIR"
    else
        echo "ComfyUI directory already exists. Skipping clone."
    fi
}

# --- Main Execution ---
echo "--- Starting ComfyUI Production Service Installation ---"
cd "$PROJECT_ROOT"

# 1. Check for Git, Python
check_dependencies

# 2. Create venv and install Python packages
setup_venv

# 3. Clone ComfyUI
install_comfyui

# 4. Install Custom Nodes
echo "--> Running custom node installation script..."
"$PYTHON_EXEC" "$INSTALL_SCRIPTS_DIR/install_custom_nodes.py"

# 5. Download Models
echo "--> Running model download script..."
if [ ! -f ".env" ]; then
    echo "WARNING: .env file not found. Model downloads might fail."
    echo "Please copy .env.example to .env and add your HUGGINGFACE_TOKEN."
fi
"$PYTHON_EXEC" "$INSTALL_SCRIPTS_DIR/install_models.py"

echo ""
echo "--- Installation Complete! ---"
echo "To activate the virtual environment, run:"
echo "source \"$VENV_DIR/bin/activate\""
echo "--------------------------------"


---


#src/install/configs/custom_nodes.txt
https://github.com/FaraamFide/ComfyUI-ParamNodes.git
https://github.com/chengzeyi/Comfy-WaveSpeed.git
https://github.com/city96/ComfyUI-GGUF.git
https://github.com/cubiq/ComfyUI_essentials.git
https://github.com/ltdrdata/ComfyUI-Impact-Pack.git
https://github.com/yolain/ComfyUI-Easy-Use.git


---


#src/install/configs/models.ini
# --- Model Configuration for install_models.py ---
#
# This file specifies which models to download from Hugging Face.
#
# === Target Folder ===
# Section names [section] determine the download folder inside 'ComfyUI/models/'.
# For example, models under [loras] will be saved to 'ComfyUI/models/loras/'.
#
# === File Naming Rules ===
# There are two ways to specify the output filename:
#
# 1. Custom Name: `my_name = <url>`
#    Saves the file as `my_name` while keeping the original extension.
#
# 2. Original Name: `_ = <url>`
#    Saves the file using its original name from the URL.




[unet]
_ = https://huggingface.co/city96/FLUX.1-dev-gguf/resolve/main/flux1-dev-Q4_K_S.gguf
#_ = https://huggingface.co/city96/FLUX.1-dev-gguf/resolve/main/flux1-dev-Q8_0.gguf

#_ = https://huggingface.co/city96/FLUX.1-schnell-gguf/resolve/main/flux1-schnell-Q4_K_S.gguf
#_ = https://huggingface.co/city96/FLUX.1-schnell-gguf/resolve/main/flux1-schnell-Q8_0.gguf



[vae]
_ = https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/ae.safetensors


[text_encoders]
_ = https://huggingface.co/city96/t5-v1_1-xxl-encoder-gguf/resolve/main/t5-v1_1-xxl-encoder-f16.gguf
#_ = https://huggingface.co/city96/t5-v1_1-xxl-encoder-gguf/resolve/main/t5-v1_1-xxl-encoder-f32.gguf

[clip]
clip_l = https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/text_encoder/model.safetensors

[loras]
Flux-Ghibsky-Illustration = https://huggingface.co/aleksa-codes/flux-ghibsky-illustration/resolve/main/lora_v2.safetensors
Flux-Ghibli-Art = https://huggingface.co/strangerzonehf/Flux-Ghibli-Art-LoRA/resolve/main/Ghibli-Art.safetensors


---


#src/install/scripts/install_models.py
import os
import configparser
import sys
from huggingface_hub import hf_hub_download
from dotenv import load_dotenv

# --- Paths ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
DOTENV_PATH = os.path.join(PROJECT_ROOT, '.env')
MODELS_PATH = os.path.join(PROJECT_ROOT, 'ComfyUI', 'models')
CONFIG_FILE = os.path.join(PROJECT_ROOT, 'install', 'configs', 'models.ini')

# --- Main Logic ---
def parse_hf_url(url):
    """Extracts repo_id and filename from a Hugging Face URL."""
    if not url.startswith("https://huggingface.co/"):
        raise ValueError("URL is not a valid Hugging Face URL.")
    
    parts = url.replace("https://huggingface.co/", "").split("/")
    repo_id = "/".join(parts[:2])
    
    try:
        resolve_index = parts.index('resolve')
        filename = "/".join(parts[resolve_index + 2:])
    except ValueError:
        raise ValueError("Cannot determine filename. Use '.../resolve/main/...' format.")
        
    return repo_id, filename

def main():
    """Parses models.ini and downloads files from Hugging Face."""
    load_dotenv(DOTENV_PATH)
    hf_token = os.getenv("HUGGINGFACE_TOKEN")

    if not os.path.exists(CONFIG_FILE):
        print(f"ERROR: Models config file not found at {CONFIG_FILE}")
        sys.exit(1)
        
    if not hf_token:
        print("WARNING: HUGGINGFACE_TOKEN not found in .env file.")
        print("Downloads may fail for private models or due to rate limits.")

    config = configparser.ConfigParser(strict=False)
    config.read(CONFIG_FILE)

    for section in config.sections():
        target_dir = os.path.join(MODELS_PATH, section)
        os.makedirs(target_dir, exist_ok=True)
        print(f"\n--- Processing section: [{section}] ---")
        print(f"Target directory: {target_dir}")

        for key, url in config.items(section):
            try:
                repo_id, remote_filename_path = parse_hf_url(url)
                original_filename = os.path.basename(remote_filename_path)

                # Determine the local filename based on the key
                if key == '_':
                    # Use the original filename from the URL
                    local_filename = original_filename
                else:
                    # Use the key as the new filename, preserving the extension
                    _ , extension = os.path.splitext(original_filename)
                    local_filename = key + extension
                
                final_path = os.path.join(target_dir, local_filename)

                if os.path.exists(final_path):
                    print(f"Model '{local_filename}' already exists. Skipping.")
                    continue

                print(f"Downloading '{remote_filename_path}' from repo '{repo_id}'...")
                
                # hf_hub_download returns the path to the downloaded file (with its original name)
                downloaded_path = hf_hub_download(
                    repo_id=repo_id,
                    filename=remote_filename_path,
                    local_dir=target_dir,
                    local_dir_use_symlinks=False, # Download the file directly
                    token=hf_token
                )

                # Rename the file if a custom name was specified
                if downloaded_path != final_path:
                    print(f"Renaming '{os.path.basename(downloaded_path)}' to '{local_filename}'...")
                    os.rename(downloaded_path, final_path)
                
                print(f"Successfully downloaded and saved as '{local_filename}'.")

            except Exception as e:
                print(f"ERROR: Failed to download from URL '{url}'. Details: {e}")
    
    print("\n--- Model download process finished. ---")

if __name__ == "__main__":
    main()



---


#src/intstall/scripts/install_custom_nodes.py
import os
import subprocess
import sys
from urllib.parse import urlparse

# --- Paths ---
# This script is in install/scripts/, so we go up two levels to the project root.
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
COMFYUI_PATH = os.path.join(PROJECT_ROOT, 'ComfyUI')
CUSTOM_NODES_PATH = os.path.join(COMFYUI_PATH, 'custom_nodes')
NODE_LIST_FILE = os.path.join(PROJECT_ROOT, 'install', 'configs', 'custom_nodes.txt')

def run_command(cmd, cwd=None):
    print(f"Executing: {' '.join(cmd)}")
    env = os.environ.copy()
    env['GIT_TERMINAL_PROMPT'] = '0'  # Отключаем интерактивный ввод git
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=cwd, env=env, timeout=300)
    except subprocess.TimeoutExpired:
        print("ERROR: Command timed out")
        sys.exit(1)

    if result.returncode != 0:
        print(f"ERROR: Command failed with exit code {result.returncode}")
        print(f"STDOUT: {result.stdout.strip()}")
        print(f"STDERR: {result.stderr.strip()}")
        sys.exit(1)
    return result

def main():
    """Clones node repos and installs their dependencies."""
    if not os.path.exists(NODE_LIST_FILE):
        print(f"ERROR: Node list file not found at {NODE_LIST_FILE}")
        sys.exit(1)

    os.makedirs(CUSTOM_NODES_PATH, exist_ok=True)

    with open(NODE_LIST_FILE, 'r') as f:
        urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]

    for url in urls:
        if not url.endswith('.git'):
            url += '.git'

        repo_name = os.path.splitext(os.path.basename(urlparse(url).path))[0]
        target_dir = os.path.join(CUSTOM_NODES_PATH, repo_name)

        if os.path.exists(target_dir):
            print(f"Node '{repo_name}' already exists. Skipping.")
            continue

        print(f"Cloning '{url}' into '{target_dir}'...")
        run_command(['git', 'clone', url, target_dir])

        # Check for and install requirements for the cloned node
        requirements_path = os.path.join(target_dir, 'requirements.txt')
        if os.path.exists(requirements_path):
            print(f"Found requirements.txt for '{repo_name}'. Installing...")
            # Use sys.executable to ensure we use the pip from the correct venv
            pip_cmd = [sys.executable, '-m', 'pip', 'install', '-r', requirements_path]
            run_command(pip_cmd)

        print("-" * 40)

    print("Custom node installation complete.")

if __name__ == "__main__":
    main()


